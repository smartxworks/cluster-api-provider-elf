---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: "${CLUSTER_NAME}"
  namespace: "${NAMESPACE}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  clusterNetwork:
    pods:
      cidrBlocks: ["172.16.0.0/16"]
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: ElfCluster
    name: "${CLUSTER_NAME}"
  controlPlaneRef:
    kind: KubeadmControlPlane
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    name: "${CLUSTER_NAME}-control-plane"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ElfCluster
metadata:
  name: "${CLUSTER_NAME}"
  namespace: "${NAMESPACE}"
spec:
  cluster: "${ELF_CLUSTER}"
  tower:
    server: "${TOWER_SERVER}"
    username: "${TOWER_USERNAME}"
    password: "${TOWER_PASSWORD}"
    authMode: ${TOWER_AUTH_MODE:=LOCAL}
    skipTLSVerify: ${TOWER_SKIP_TLS_VERIFY:=false}
  controlPlaneEndpoint:
    host: "${CONTROL_PLANE_ENDPOINT_IP}"
    port: 6443
  vmGracefulShutdown: ${VM_GRACEFUL_SHUTDOWN:=false}

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ElfMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-control-plane"
  namespace: "${NAMESPACE}"
spec:
  template:
    spec:
      osType: LINUX
      template: "${VM_TEMPLATE}"
      ha: true
      cloneMode: ${ELF_VM_CLONE_MODE:-FastClone}
      numCPUS: ${CONTROL_PLANE_MACHINE_NUM_CPUS:-2}
      memoryMiB: ${CONTROL_PLANE_MACHINE_MEMORY_MB:-4096}
      diskGiB: ${CONTROL_PLANE_MACHINE_DISK_GB:-0}
      network:
        nameservers: []
        devices:
        - networkType: IPV4_DHCP
          vlan: "${ELF_VLAN}"
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: "${CLUSTER_NAME}-control-plane"
  namespace: "${NAMESPACE}"
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  version: "${KUBERNETES_VERSION}"
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: ElfMachineTemplate
      name: "${CLUSTER_NAME}-control-plane"
      namespace: "${NAMESPACE}"
    metadata: {}
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 1
    type: RollingUpdate
  kubeadmConfigSpec:
    clusterConfiguration:
      clusterName: "${CLUSTER_NAME}"
      imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
      apiServer:
        extraArgs:
      controllerManager:
        extraArgs:
    format: cloud-config
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
        name: '{{ ds.meta_data.hostname }}'
    preKubeadmCommands:
      - hostname "{{ ds.meta_data.hostname }}"
      - echo "::1         ipv6-localhost ipv6-loopback" >/etc/hosts
      - echo "127.0.0.1   localhost" >>/etc/hosts
      - echo "127.0.0.1   {{ ds.meta_data.hostname }}" >>/etc/hosts
      - echo "{{ ds.meta_data.hostname }}" >/etc/hostname
      - /etc/kube-vip-prepare.sh
      #! If you need to expand the system disk capacity (based on the capacity of
      #! the virtual machine template disk) when creating a node virtual machine,
      #! you need to set the following commands. CAPE will add new disk capacity
      #! to the system disk through Tower API. These commands will add the new
      #! capacity of the system disk to the root partition.
      #! - |
      #!   . /etc/os-release
      #!   rootpath="/dev/mapper/rl-root"
      #!   if [[ $ID == 'openEuler' ]]; then
      #!     rootpath="/dev/mapper/openeuler-root"
      #!   fi
      #! - |
      #!   result=$(growpart /dev/vda 2)
      #!   if [[ $? == 0 ]]; then
      #!     echo "$result"
      #!   elif [[ $result == NOCHANGE* ]]; then
      #!     echo "$result"
      #!   else
      #!     echo "$result"
      #!     exit 1
      #!   fi
      #! - "pvresize /dev/vda2"
      #! - |
      #!   result=$(lvextend -l+100%FREE -n $rootpath 2>&1)
      #!   if [[ $? == 0 ]]; then
      #!     echo "$result"
      #!   elif [[ $result == *'matches existing size'* ]]; then
      #!     echo "$result"
      #!   else
      #!     echo "$result"
      #!     exit 1
      #!   fi
    useExperimentalRetryJoin: true
    files:
    - content: |
        apiVersion: v1
        kind: Pod
        metadata:
          creationTimestamp: null
          name: kube-vip
          namespace: kube-system
        spec:
          containers:
          - args:
            - manager
            env:
            - name: vip_arp
              value: "true"
            - name: port
              value: "6443"
            - name: vip_interface
              value: ${VIP_NETWORK_INTERFACE:="ens4"}
            - name: cp_enable
              value: "true"
            - name: cp_namespace
              value: kube-system
            - name: vip_ddns
              value: "false"
            - name: vip_leaderelection
              value: "true"
            - name: vip_leasename
              value: plndr-cp-lock
            - name: vip_leaseduration
              value: "15"
            - name: vip_renewdeadline
              value: "10"
            - name: vip_retryperiod
              value: "2"
            - name: address
              value: ${CONTROL_PLANE_ENDPOINT_IP}
            - name: prometheus_server
              value: :2112
            image: ghcr.io/kube-vip/kube-vip:v0.7.1
            imagePullPolicy: IfNotPresent
            name: kube-vip
            resources: {}
            securityContext:
              capabilities:
                add:
                - NET_ADMIN
                - NET_RAW
            volumeMounts:
            - mountPath: /etc/kubernetes/admin.conf
              name: kubeconfig
            - mountPath: /etc/localtime
              name: localtime
              readOnly: true
          hostAliases:
          - hostnames:
            - kubernetes
            ip: 127.0.0.1
          hostNetwork: true
          volumes:
          - hostPath:
              path: /etc/kubernetes/admin.conf
            name: kubeconfig
          - hostPath:
              path: /etc/localtime
              type: ""
            name: localtime
        status: {}
      owner: root:root
      path: /etc/kubernetes/manifests/kube-vip.yaml
    - content: |
        #!/bin/bash

        # Copyright 2020 The Kubernetes Authors.
        #
        # Licensed under the Apache License, Version 2.0 (the "License");
        # you may not use this file except in compliance with the License.
        # You may obtain a copy of the License at
        #
        #     http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.

        set -e

        # Configure the workaround required for kubeadm init with kube-vip:
        # xref: https://github.com/kube-vip/kube-vip/issues/684

        # Nothing to do for kubernetes < v1.29
        KUBEADM_MINOR="$(kubeadm version -o short | cut -d '.' -f 2)"
        if [[ "$KUBEADM_MINOR" -lt "29" ]]; then
          exit 0
        fi

        IS_KUBEADM_INIT="false"

        # cloud-init kubeadm init
        if [[ -f /run/kubeadm/kubeadm.yaml ]]; then
          IS_KUBEADM_INIT="true"
        fi

        # ignition kubeadm init
        if [[ -f /etc/kubeadm.sh ]] && grep -q -e "kubeadm init" /etc/kubeadm.sh; then
          IS_KUBEADM_INIT="true"
        fi

        if [[ "$IS_KUBEADM_INIT" == "true" ]]; then
          sed -i 's#path: /etc/kubernetes/admin.conf#path: /etc/kubernetes/super-admin.conf#' \
            /etc/kubernetes/manifests/kube-vip.yaml
        fi
      owner: root:root
      path: /etc/kube-vip-prepare.sh
      permissions: "0700"
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
  namespace: "${NAMESPACE}"
spec:
  template:
    spec:
      clusterConfiguration:
        clusterName: "${CLUSTER_NAME}"
        imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
      format: cloud-config
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
          name: '{{ ds.meta_data.hostname }}'
      preKubeadmCommands:
        - hostname "{{ ds.meta_data.hostname }}"
        - echo "::1         ipv6-localhost ipv6-loopback" >/etc/hosts
        - echo "127.0.0.1   localhost" >>/etc/hosts
        - echo "127.0.0.1   {{ ds.meta_data.hostname }}" >>/etc/hosts
        - echo "{{ ds.meta_data.hostname }}" >/etc/hostname
      #! If you need to expand the system disk capacity (based on the capacity of
      #! the virtual machine template disk) when creating a node virtual machine,
      #! you need to set the following commands. CAPE will add new disk capacity
      #! to the system disk through Tower API. These commands will add the new
      #! capacity of the system disk to the root partition.
      #! - |
      #!   . /etc/os-release
      #!   rootpath="/dev/mapper/rl-root"
      #!   if [[ $ID == 'openEuler' ]]; then
      #!     rootpath="/dev/mapper/openeuler-root"
      #!   fi
      #! - |
      #!   result=$(growpart /dev/vda 2)
      #!   if [[ $? == 0 ]]; then
      #!     echo "$result"
      #!   elif [[ $result == NOCHANGE* ]]; then
      #!     echo "$result"
      #!   else
      #!     echo "$result"
      #!     exit 1
      #!   fi
      #! - "pvresize /dev/vda2"
      #! - |
      #!   result=$(lvextend -l+100%FREE -n $rootpath 2>&1)
      #!   if [[ $? == 0 ]]; then
      #!     echo "$result"
      #!   elif [[ $result == *'matches existing size'* ]]; then
      #!     echo "$result"
      #!   else
      #!     echo "$result"
      #!     exit 1
      #!   fi
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ElfMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-worker"
  namespace: "${NAMESPACE}"
spec:
  template:
    spec:
      osType: LINUX
      template: "${VM_TEMPLATE}"
      ha: true
      cloneMode: ${ELF_VM_CLONE_MODE:-FastClone}
      numCPUS: ${WORKER_MACHINE_NUM_CPUS:-2}
      memoryMiB: ${WORKER_MACHINE_MEMORY_MB:-4096}
      diskGiB: ${WORKER_MACHINE_DISK_GB:-0}
      network:
        nameservers: []
        devices:
        - networkType: IPV4_DHCP
          vlan: "${ELF_VLAN}"
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "${CLUSTER_NAME}-md-0"
  namespace: "${NAMESPACE}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  clusterName: "${CLUSTER_NAME}"
  minReadySeconds: 0
  progressDeadlineSeconds: 600
  replicas: ${WORKER_MACHINE_COUNT}
  revisionHistoryLimit: 1
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
      cluster.x-k8s.io/deployment-name: "${CLUSTER_NAME}-md-0"
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
        cluster.x-k8s.io/deployment-name: "${CLUSTER_NAME}-md-0"
    spec:
      clusterName: "${CLUSTER_NAME}"
      version: "${KUBERNETES_VERSION}"
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: "${CLUSTER_NAME}-md-0"
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: ElfMachineTemplate
        name: "${CLUSTER_NAME}-worker"
