---
# E2E test scenario using local dev images and manifests built from the source tree for following providers:
# - cluster-api
# - bootstrap kubeadm
# - control-plane kubeadm
# - elf

# For creating local dev images built from the source tree;
# - from the CAPI repository root, `make docker-build REGISTRY=gcr.io/k8s-staging-cluster-api` to build the cluster-api,
#  bootstrap kubeadm, control-plane kubeadm provider images. This step can be skipped to use upstream images.
# - from the CAPE repository root, `make e2e` to build the elf provider image and run e2e tests.

images:
  - name: k8s.gcr.io/cluster-api/cluster-api-controller:v1.2.0
    loadBehavior: tryLoad
  - name: k8s.gcr.io/cluster-api/kubeadm-bootstrap-controller:v1.2.0
    loadBehavior: tryLoad
  - name: k8s.gcr.io/cluster-api/kubeadm-control-plane-controller:v1.2.0
    loadBehavior: tryLoad
  - name: k8s.gcr.io/smartxworks/cape-manager:e2e
    loadBehavior: mustLoad
  - name: quay.io/jetstack/cert-manager-cainjector:v1.8.2
    loadBehavior: tryLoad
  - name: quay.io/jetstack/cert-manager-webhook:v1.8.2
    loadBehavior: tryLoad
  - name: quay.io/jetstack/cert-manager-controller:v1.8.2
    loadBehavior: tryLoad

providers:
- name: cluster-api
  type: CoreProvider
  versions:
    - name: v1.2.0 # Use manifest from source files
      value: "https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.2.0/core-components.yaml"
      type: "url"
      contract: v1beta1
      files:
      - sourcePath: "../data/capi/metadata.yaml"
      replacements:
      - old: "imagePullPolicy: Always"
        new: "imagePullPolicy: IfNotPresent"
      - old: --metrics-addr=127.0.0.1:8080
        new: --metrics-addr=:8080

- name: kubeadm
  type: BootstrapProvider
  files:
  - sourcePath: "../data/capi/metadata.yaml"
  versions:
  - name: v1.2.0 # Use manifest from source files
    value: "https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.2.0/bootstrap-components.yaml"
    type: "url"
    contract: v1beta1
    files:
    - sourcePath: "../data/capi/metadata.yaml"
    replacements:
    - old: "imagePullPolicy: Always"
      new: "imagePullPolicy: IfNotPresent"
    - old: --metrics-addr=127.0.0.1:8080
      new: --metrics-addr=:8080

- name: kubeadm
  type: ControlPlaneProvider
  files:
  - sourcePath: "../data/capi/metadata.yaml"
  versions:
  - name: v1.2.0 # Use manifest from source files
    value: "https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.2.0/control-plane-components.yaml"
    type: "url"
    contract: v1beta1
    files:
    - sourcePath: "../data/capi/metadata.yaml"
    replacements:
    - old: "imagePullPolicy: Always"
      new: "imagePullPolicy: IfNotPresent"
    - old: --metrics-addr=127.0.0.1:8080
      new: --metrics-addr=:8080

- name: elf
  type: InfrastructureProvider
  versions:
  - name: v0.3.4
  # Use manifest from source files
    value: ../../../../cluster-api-provider-elf/config/default
    contract: v1beta1
    replacements:
    - old: docker.io/smartxworks/cape-manager:latest
      new: k8s.gcr.io/smartxworks/cape-manager:e2e
    - old: "imagePullPolicy: Always"
      new: "imagePullPolicy: IfNotPresent"
  files:
  # Add a cluster template
  - sourcePath: "../../../test/e2e/data/infrastructure-elf/cluster-template.yaml"
  - sourcePath: "../../../test/e2e/data/infrastructure-elf/cluster-template-cp-ha.yaml"
  - sourcePath: "../../../test/e2e/data/infrastructure-elf/cluster-template-kcp-remediation.yaml"
  - sourcePath: "../../../test/e2e/data/infrastructure-elf/cluster-template-kcp-scale-in.yaml"
  - sourcePath: "../../../test/e2e/data/infrastructure-elf/cluster-template-md-remediation.yaml"
  - sourcePath: "../../../test/e2e/data/infrastructure-elf/cluster-template-node-drain.yaml"
  - sourcePath: "../../../test/e2e/data/infrastructure-elf/cluster-template-conformance.yaml"
  - sourcePath: "../data/cape/metadata.yaml"

variables:
  KUBERNETES_VERSION: "v1.20.6"
  KUBERNETES_VERSION_UPGRADE_TO: "v1.21.2"
  KUBERNETES_VERSION_UPGRADE_FROM: "v1.20.6"
  ETCD_VERSION_UPGRADE_TO: "3.4.13-0"
  COREDNS_VERSION_UPGRADE_TO: "v1.8.0"
  # CONTROL_PLANE_MACHINE_COUNT: 1
  # WORKER_MACHINE_COUNT: 1
  CNI: "./data/cni/calico/calico.yaml"
  EXP_CLUSTER_RESOURCE_SET: "true"
  IP_FAMILY: "IPv4"
  NODE_DRAIN_TIMEOUT: "60s"
  # Following CAPE variables should be set before testing
  ELF_TEMPLATE: "336820d7-5ba5-4707-9d0c-8f3e583b950f"
  ELF_TEMPLATE_UPGRADE_TO: "c1347c27-ddd7-4b97-82e3-ca4e124623b4"
  CONTROL_PLANE_ENDPOINT_IP: "127.0.0.1"
  ELF_VLAN: "576ad467-d09e-4235-9dec-b615814ddc7e_c8a1e42d-e0f3-4d50-a190-53209a98f157"
  ELF_CLUSTER: "576ad467-d09e-4235-9dec-b615814ddc7e"
  # Also following variables are required but it is recommended to use env variables to avoid disclosure of sensitive data
  # TOWER_SERVER: "127.0.0.1"
  # TOWER_USERNAME: "root"
  # TOWER_PASSWORD: "root"
  # TOWER_AUTH_MODE: "LOCAL"
  # TOWER_TLS_SKIP_VERIFY: false

intervals:
  default/wait-controllers: ["8m", "10s"]
  default/wait-cluster: ["8m", "10s"]
  default/wait-control-plane: ["10m", "10s"]
  default/wait-worker-nodes: ["8m", "10s"]
  default/wait-delete-cluster: ["8m", "10s"]
  default/wait-machine-upgrade: ["15m", "1m"]
  default/wait-nodes-ready: ["10m", "10s"]
  default/wait-machine-remediation: ["8m", "10s"]
  default/wait-vm-job: ["3m", "10s"]
  node-drain/wait-deployment-available: [ "3m", "10s" ]
  node-drain/wait-machine-deleted: [ "3m", "10s" ]
